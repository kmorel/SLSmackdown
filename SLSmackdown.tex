% -*- latex -*-

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This beginning part of the preamble is specific to the vgtc document class.

\documentclass{vgtc}                          % final (conference style)
%\documentclass[review]{vgtc}                 % review
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint]{vgtc}               % preprint
%\documentclass[electronic]{vgtc}             % electronic version

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Figures should be in CMYK or Grey scale format, otherwise, colour 
%% shifting may occur during the printing process.

%% These three lines bring in essential packages: ``mathptmx'' for Type 1 
%% typefaces, ``graphicx'' for inclusion of EPS figures. and ``times''
%% for proper handling of the times font family.

\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{times}

%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
\vgtcinsertpkg

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% Paper title.

\title{Sort-Last Smackdown!}

%% This is how authors are specified in the conference style

%% Author and Affiliation (single author).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}}
%%\affiliation{\scriptsize Allied Widgets Research}

%% Author and Affiliation (multiple authors with single affiliations).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com} %
%%\and Ed Grimley\thanks{e-mail:ed.grimley@aol.com} %
%%\and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}}
%%\affiliation{\scriptsize Martha Stewart Enterprises \\ Microsoft Research}

%% Author and Affiliation (multiple authors with multiple affiliations)
%% \author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}\\ %
%%         \scriptsize Starbucks Research %
%% \and Ed Grimley\thanks{e-mail:ed.grimley@aol.com}\\ %
%%      \scriptsize Grimley Widgets, Inc. %
%% \and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}\\ %
%%      \parbox{1.4in}{\scriptsize \centering Martha Stewart Enterprises \\ Microsoft Research}}

\author{ %
  Kenneth Moreland\thanks{e-mail: kmorel@sandia.gov} \\ %
    \scriptsize Sandia National Laboratories %
  \and Et All\thanks{e-mail: et.all@elsewhere.edu} \\ %
    \scriptsize Elsewhere %
}

%% A teaser figure can be included as follows, but is not recommended since
%% the space is now taken up by a full width abstract.
%\teaser{
%  \includegraphics[width=1.5in]{sample.eps}
%  \caption{Lookit! Lookit!}
%}

%% Abstract section
\abstract{ The only proven method for performing distributed-memory
  parallel rendering at large scales, tens of thousands of nodes, is a
  class of algorithms called sort last.  The fundamental operation of
  sort-last parallel rendering is an image composite, which combines a
  collection of images generated independently on each node into a single
  blended image.  Over the years numerous image compositing algorithms have
  been proposed as well as several enhancements and rendering modes to
  these core algorithms.  However, the testing of these image compositing
  algorithms has been with an arbitrary set of enhancements, if any are
  applied at all.  In this paper we take a leading production-quality
  image-compositing framework, IceT, and use it as a testing framework for
  the leading image compositing algorithms of today.  The compositing
  enhancements provided by IceT, including some introduced in this paper,
  are employed in our measurements.  IceT also provides different
  compositing mechanisms for different rendering environments such as
  opaque surface versus volume rendering and fixed point versus floating
  point color representations.  These variations are also considered in our
  analysis.  To understand the behavior of these algorithms at vary large
  scale, we run tests on up to \sticky{XXXX} cores of the Intrepid BlueGene/P at
  Argonne National Laboratories.  }

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
%% The ``\CCScat'' command takes four arguments.

\CCScatlist{
  \CCScat{I.3.1}{Computer Graphics}{Hardware Architecture}{Parallel processing}
}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

% End of vgtc-specific portion of the preamble.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{varioref}
\usepackage{fancyvrb}
\usepackage{ifthen}
\usepackage{cite}
\usepackage{subfigure}
\usepackage{xspace}
\usepackage{hyperref}

\usepackage{color}
\definecolor{yellow}{rgb}{1,1,0}
\definecolor{black}{rgb}{0,0,0}
\definecolor{ltcyan}{rgb}{.75,1,1}
\definecolor{red}{rgb}{1,0,0}

% Cite commands I use to abstract away the different ways to reference an
% entry in the bibliography (superscripts, numbers, dates, or author
% abbreviations).  \scite is a short cite that is used immediately after
% when the authors are mentioned.  \lcite is a full citation that is used
% anywhere.  Both should be used right next to the text being cited without
% any spacing.
\newcommand*{\lcite}[1]{~\cite{#1}}
\newcommand*{\scite}[1]{~\cite{#1}}

\newcommand{\etal}{et al.}

\newcommand*{\keyterm}[1]{\emph{#1}}

\newcommand{\Oh}{\mathrm{O}}

\newcommand{\sticky}[1]{{\color{red}\textsc{[#1]}}}

\begin{document}

%% VGTC-specific:
%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}

\maketitle

%% \section{Introduction} 
\label{sec:Introduction}

The demands of parallel rendering continue to grow as visualization is
applied to ever larger scientific data.  Early efforts have satisfied the
need of parallel rendering on specialized visualization clusters containing
hundreds of nodes.  Because of recent constraints in building specialized
visualization clusters\lcite{Childs2007}, recent research focuses on
performing visualization directly on the same supercomputing architectures
driving simulation.  Whereas previous research considers rendering on the
order of hundreds of processes, recent efforts use over ten thousand
processes\lcite{Childs2010}.  Furthermore, in an attempted to get around
bottlenecks introduced by file I/O, it is now more commonplace to have
visualization run \emph{in-situ} with
simulation\lcite{Ma2009:SciDACReview,Ma2009:CG&A,Yu2010,Tu2006}, meaning
parallel rendering will soon run on over a hundred thousand processes.

These increased demands on parallel rendering have spawned a resurgence in
parallel rendering research.  Recent studies investigate the scaling of
parallel rendering algorithms\lcite{Peterka2009}, the creation of new image
compositing algorithms\lcite{23Swap,RadixK}, and new compositing
enhancements\lcite{Kendall2010}.  Although each of these studies improve
the state of the art in parallel rendering, all involve locally built
algorithm implementations that contain some arbitrary subset of
enhancements and that may or may not be publicly available.  The
consequence is that it is difficult to repeat the experiments, to compare
the results with each other, and to apply the algorithms to production
software.

The intention of this work is to bridge the gap between independent
parallel rendering algorithm development and practical application by
bringing together multiple algorithms and enhancements together in a
production quality parallel rendering library.  More specifically, this
paper provides the following.

\begin{itemize}
\item An introduction to a sort-last rendering framework named
  \keyterm{IceT} that is general purpose, production quality, and fully
  optimized.
\item Describe an optimization to the compositing order for radix-k and
  direct send that minimizes the number of copies of non-overlapping
  pixels.
\item The \keyterm{telescoping} algorithm, which can be applied to an
  existing image compositing algorithm that works best on powers of two,
  such as binary swap, to run efficiently on any number of processes.
\item A new method of \keyterm{image interlacing} that requires no
  additional image copying to reconstruct the final image.
\item A comparison of the ever popular binary-swap algorithm with the newer
  radix-k algorithm on leadership-class high-performance computers.  These
  tests are performed with every enhancement one should expect in
  production quality parallel rendering as well as with a variety of
  rendering modes that can be encountered in production software.
\item An investigation of the collection operation required at the end of
  compositing and adjustments to the compositing algorithms to minimize
  it.
\item An investigation comparing the performance of multi-tile compositing
  techniques\lcite{Moreland2001} with the best single image compositing
  techniques for single images.
\end{itemize}

\section{Previous Work}
\label{sec:PreviousWork}

Although many aspects of parallel rendering have changed since the sorting
classification of parallel rendering algorithms was
introduced\lcite{Molnar1994}, these classifications are still used today
because they accurately characterize and predict the scaling performance of
these algorithms.  When rendering on a hundred or more distributed nodes,
the most efficient class of algorithm is sort-last because it scales
extremely well with respect to the number of processes and size of the
geometry being rendered and because the main contributing factor to its
overhead, the size of the image being rendered, is fixed by the display
that we are using\lcite{Wylie2001}.

The main characteristics of sort-last parallel rendering is that geometry
is statically partitioned; processes each independently render images using
only their local partition, and these images are \keyterm{composited}
together by blending or comparing pixels.  Consequently, it is the behavior
of this compositing operation that determines the overall efficiency of
sort-last parallel rendering.

\subsection{Basic Parallel Compositing Algorithms}
\label{sec:BasicParallelCompositingAlgorithms}

Over the years researchers have designed several variations of the image
compositing algorithm.  One of the oldest and simplest algorithms that is
still in wide use is direct send\lcite{DirectSend1,DirectSend2}.  Direct
send assigns each process a unique partition of the image to be rendered.
After the local geometry is rendered, each process sends each pixel
fragment directly to the process responsible for compositing it.  Each
process then collects pixel fragments from all other processes and combines
them to form its partition of the image.  Although direct send is efficient
in the amount of data it transfers, the number of messages it generates
grows quadratically with the number of processes.  Thus, for large numbers
of processes the network can get overwhelmed by many small messages.

One of the most popular image compositing algorithms is binary
swap\lcite{BinarySwap1,BinarySwap2}.  Binary swap executes in rounds.
During a round, each process pairs up with another process, the image is
split in half, the paired processes exchange image halves, and each process
composites the pixel fragments for the half of the image it received.
After $\log_{2} n$ rounds, where $n$ is the number of processes, each
process holds a unique fully-composited partition of the image.  Binary
swap uses fewer messages than direct send: $n \log_{2} n$ total messages
with only $n$ messages at a time (assuming minimal overlap between
rounds).  Because the bisection bandwidth of a cluster interconnect
generally grows with respect to the number of nodes, the bandwidth
requirements on the network remain relatively fixed.

One of the problems with sort last is that it requires a number of
processes equal to a power of two.  The simplest solution in dealing with
other process counts is to \keyterm{fold} the images into a group of the
correct size.  Create the largest group possible with a power of two, and
then send the image data from those processes outside the group to a
process inside the group.  Those processes outside the group sit idle while
those inside the group continue on to composite the image.  This approach
has inefficiencies because processes have to sit idle during most of the
computation.  The 2-3 swap algorithm\lcite{23Swap} takes a different
approach.  It relaxes binary swap such that processes can be grouped into
pairs of two (like binary swap) or sets of three (unlike binary swap).
Using these groups of two or three, 2-3 swap can decompose any number of
processes into groups, and in this way all processes can take part in
compositing at all times.

Radix-k\lcite{RadixK} is a combination of binary swap and direct send.
Radix-k first factors the number of processes into a series of what are
called $k$ values, which need not be prime.  In a sequence of rounds, one
per $k$ value, radix-k partitions the processes into groups of size $k$ and
performs a direct send within each group.  The next round recurses into
processes with the same partition until all $k$ values are used and each
process has a unique partition.  When it has one round with a $k$ value
equal to the number of processes, radix-k is equivalent to direct send.
When it has $\log_{2} n$ rounds with all $k$ values equal to two, radix-k
is equivalent to binary swap.

Radix-k improves on binary swap by overlapping data transfers with
computation.  When receiving data from multiple processes, which happens
whenever $k$ is greater than two, radix-k can begin compositing pixels as
soon as the first message is received while other messages are still in
transit.  Yet radix-k retains binary swap's ability to limit the total
number of messages sent.  Radix-k is also able to handle process groups
that are not powers of two because the $k$ value for each round can be any
factor.  That said, if the number of processes factors into large prime
numbers, the performance can degrade to that of direct send.

\subsection{Compositing Enhancements}
\label{sec:CompositingEnhancements}

A na\"{i}ve implementation of sort-last image compositing will consider
every pixel fragment from every process participating.  However, in almost
all practical use cases the geometry is, or at least can be, partitioned
spatially.  When each process has geometry in a confined spatial region,
there is a great deal of empty space in the original rendered images.  A
pragmatic image compositing algorithm takes advantage of these empty spaces
in two ways.  First, the pixels in these empty regions will be removed from
communication, thus making better use of available network bandwidth.
Second, the empty regions are not considered in the composite operation,
which reduces the overall computation performed.

There are two standard approaches for tracking the ``active'' pixels (those
that have been rendered to) and ``inactive'' pixels (those over empty
regions).  The first method is to track bounding boxes around geometry.
Typically, a box around the geometry in each process is projected to screen
space to define the region of pixels that likely have geometry rendered to
them.  (The boxes are often expanded to axis aligned bounding boxes to
simplify management.)  Only the pixels in this region are read,
transferred, and composited.  Ma \etal\scite{BinarySwap2} show that in the
common case tracking bounding boxes reduces the total number of pixels
transmitted from $\Oh(n p)$ to $\Oh(n^{1/3} p)$, where $n$ and $p$ are the
number of processes and pixels, respectively.

\sticky{If the paper is long we can shorten it by removing the discussion
  on active pixels.}

The second approach for tracking active and inactive pixels is to use
run-length encoding\lcite{Ahrens1998}.  A generic run-length encoder
will look for run lengths of any repeated value.  However, when compositing
images the active pixels tend to have run lengths of 1, so run-length
encoding can actually hurt in these regions.  Thus, a better approach is to
use \keyterm{active pixel encoding}, which classifies the pixels as either
active or inactive and provide run lengths for continuous regions of any
active pixels.  Moreland \etal\scite{Moreland2001} show that this
encoding is both effective and never adds to the data size even in the
worst pathological cases.  Active pixel encoding improves on region boxes
by tightly identifying active and inactive pixels.  There is a greater
overhead incurred by searching through the image for inactive pixels, but
this overhead is mitigated by considering the bounding boxes during the
initial encoding\lcite{Yang1999}.

Although active pixel encoding almost always improves the performance of
compositing, it does introduce an issue of load balancing.  As images are
partitioned, some regions will have more active pixels than others.  By
balancing the active pixels assigned to regions, the parallel compositing
becomes better load balanced and performance can improve even further.

The most straightforward way of balancing active pixels is to
\keyterm{interlace} the
images\lcite{Molnar1994,Takeuchi2003}.\footnote{Other literature uses the
  term interleave, but we feel the word interlace is more descriptive.}  An
image is interlaced by rearranging regions of pixels, commonly scanlines,
in a different order.  This reordering of pixels is designed such that when
the images are later partitioned, each partition gets pixels from all over
the images.  Consequently, regions with many active pixels are distributed
to all the partitions.

The SLIC algorithm\lcite{SLIC} integrates the direct-send algorithm with
inactive pixel skipping and image interlacing.  It finds areas of geometry
overlap by projecting bounding boxes to screen space.  SLIC then breaks
scanlines by areas of overlap and uses a simple hashing function to assign
these scanline fragments to processes.  The hash function provides load
balancing and the tracking of overlap limits the total number of messages
to $\Oh(n^{4/3})$, where $n$ is the number of processes, which is better
than the original direct send but worse than binary swap or radix-k.

One problem with image interlacing is that the pixels in the fully
composited region must be rearranged once again into the correct order.
This added overhead can remove the performance gains of the load balancing.
To get around this problem, Kendall \etal\scite{Kendall2010} propose a
method in which the partitioning for the radix-k algorithm is adjusted so
that each partition has the same amount of active pixels.  Although
Kendall's algorithm improves load balancing, it also adds overhead in
readjusting partitions for consistency amongst all processes.

Most sort last algorithms rely on a static partitioning of the data to
produce empty regions of the data.  Hybrid algorithms\lcite{Samanta2000}
also use dynamic partitioning of the data to collect geometry by screen
region based on the current projection.  Hybrid algorithms reduce the
compositing time at the expense of redistributing geometry, which means the
effectiveness of the technique is dependent on the amount of geometry being
rendered.  Other approaches propose ensuring empty space regions using
static partitions with replication\lcite{Samanta2001}.

\section{Testing Environment}
\label{sec:TestingEnvironment}

The Image Composition Engine for Tiles (IceT) is a high-performance
sort-last parallel rendering library\lcite{IceT}.  Although originally
created to capture sort-last rendering algorithms for tiled
displays\lcite{Moreland2001}, IceT also works effectively for smaller
single image displays.

IceT contains several image compositing algorithms, and its internal
architecture makes it straightforward to add new algorithms.  It also
optimizes the compositing process by tracking the projection of geometry
and compressing images through run-length encoding.  IceT also supports
multiple rendering modes allowing both color blending for volume rendering
and z-buffer comparisons for opaque geometries.

IceT is used in multiple production products like ParaView\lcite{ParaView}
and VisIt\lcite{VisIt} and has been used to achieve record-breaking
rendering rates\lcite{Higham2005}.  As such, IceT is an excellent code base
for creating, testing, and comparing image compositing algorithms.  It
already contains routines for efficiently capturing, compressing, and
compositing images.  It also contains efficient existing algorithms to
which to compare new ones.  Furthermore, any optimizations or new
algorithms added to IceT can be applied to existing production software.

The experiments we run for this paper are encapsulated in IceT's testing
suite under the SimpleTiming test.  This test evenly partitions volume-wise
a cube of space amongst processes.  Each process renders a hexahedron
filling the space it is assigned as a proxy geometry for the rendering.  We
use this proxy rendering to simplify compiling and porting, which should be
particularly useful for anyone wishing to repeat these experiments.  In any
case, the rendering time is discarded as we are interested only in the
compositing overhead.  Figure~\ref{fig:SimpleTimingOutput} shows an example
of images rendered by SimpleTiming.  For each SimpleTiming test we render
101 frames at pseudorandom viewpoints, always using the same seed for
consistency between experiments.  The time for the first frame is thrown
out of any average because it contains added overhead of memory allocations
not included in subsequent frames.

\begin{figure}[htb]
  \centering
  \includegraphics[width=.4\linewidth]{images/TransparentOutput}
  \quad
  \includegraphics[width=.4\linewidth]{images/OpaqueOutput}
  \caption{Examples of images rendered in our experiments.}
  \label{fig:SimpleTimingOutput}
\end{figure}

The experiments reported in this paper were run on Argonne National
Laboratory's Intrepid Blue Gene/P computer.  Intrepid comprises a total of
40,960 nodes, each containing four cores.  Each experiment was run in one
of two modes.  The first mode, \keyterm{Shared Memory Processors} (SMP),
runs a single MPI process on each Intrepid node.  The intention of the mode
is to run multiple threads to use all four cores, but in our experiments we
run a single thread using only one core.  The second mode, \keyterm{Virtual
  Node} (VN), runs four MPI processes on each Intrepid node.  It treats
each core on the node as a distributed memory processes even though it is
possible to share memory.  Data transfers amongst the processes within a
single node still require explicit MPI memory passing although the
underlying MPI layer bypasses the network infrastructure in this case.  We
consider both running modes because both are commonly used today and each
has differing demands on the underlying subsystems.

%% VGTC-specific section command.
\acknowledgements{Funding for this work was provided by the SciDAC
  Institute for Ultrascale Visualization and and by the Advanced Simulation
  and Computing Program of the National Nuclear Security Adiminstration.

  Sandia National Laboratories is a multi-program laboratory operated by
  Sandia Corporation, a wholly owned subsidiary of Lockheed Martin
  Corporation, for the U.S. Department of Energy's National Nuclear
  Security Administration.}

\bibliographystyle{abbrv}
\bibliography{SLSmackdown}

\end{document}
